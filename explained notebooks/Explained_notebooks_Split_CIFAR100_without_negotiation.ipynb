{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb588977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walsh_vector_list.shape (128, 128)\n",
      "average_acc of all classes for the current seed 0.154\n",
      "walsh_vector_list.shape (128, 128)\n",
      "average_acc of all classes for the current seed 0.132\n",
      "walsh_vector_list.shape (128, 128)\n",
      "average_acc of all classes for the current seed 0.10900000000000001\n",
      "walsh_vector_list.shape (128, 128)\n",
      "average_acc of all classes for the current seed 0.144\n",
      "walsh_vector_list.shape (128, 128)\n",
      "average_acc of all classes for the current seed 0.127\n",
      "average_of_all_seeds: 0.1332\n",
      "individiaul_averages: [0.154, 0.132, 0.10900000000000001, 0.144, 0.127]\n",
      "average_of_all_seeds: 0.1332\n",
      "individiaul_averages: [0.154, 0.132, 0.10900000000000001, 0.144, 0.127]\n"
     ]
    }
   ],
   "source": [
    "#Functions are explained\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,callbacks\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "\n",
    "#All functions...\n",
    "# Define the custom sigmoid function\n",
    "def custom_sigmoid(x):\n",
    "    \"\"\"\n",
    "    The purpose of defining a custom sigmoid function is to turn sigmoid fucntion into a function by which \n",
    "    class memberships are represented more linearly. \n",
    "    why ?: because if we work with logistic functions, even the slightest change cause a dramatical change \n",
    "    in the output. We dont want that kind of fragile models.\n",
    "    why not RELU: because relu is not bounded between (0,1).\n",
    "    The parameter 'sigmoid_softening_coeff' determines the rate of linearity. We can make the output \n",
    "    more linear by increasing this coefficient. But there is no need to flood the memory with big numbers.\n",
    "    \"\"\"\n",
    "    if sigmoid_softener==True:\n",
    "        return 1 / (1 + tf.exp(-x/sigmoid_softening_coeff))\n",
    "    else:\n",
    "        return 1 / (1 + tf.exp(-x))\n",
    "    \n",
    "# Create the model\n",
    "def create_model():\n",
    "    \n",
    "    model = keras.Sequential([keras.Input(shape=(32, 32, 3)),\n",
    "            layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
    "            layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            #layers.Dropout(0.25),\n",
    "            layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
    "            layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            #layers.Dropout(0.25),\n",
    "            layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
    "            layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            #layers.Dropout(0.25),\n",
    "            layers.Flatten(),\n",
    "            #layers.Dense(512, activation=\"relu\"),\n",
    "            #layers.Dropout(0.25),\n",
    "            layers.Dense(size_of_walsh, activation=\"sigmoid\"),])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def assign_to_new_classes(predicted_results,my_walsh_vectors):\n",
    "    \"\"\" \n",
    "    we need this function in the evaluation phase. In the evaluation phase, after we obtain the output representation\n",
    "    for one sample, we calculate the distance between the obtained representation and previously assigned class representations.\n",
    "    each distance is calculated with respect to the 'Binary Cross Entropy loss- BCE'. \n",
    "    Because we used BCE in the model, we had to use it in class assignments as well \n",
    "    \n",
    "    Note: When a new class is fed to the model, we keep track of which vector representation was given to that class.\n",
    "    that procedure is further explained in 'determine_walsh_vectors_as_output_labels' function. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    num_results=predicted_results.shape[0]\n",
    "    new_results=predicted_results.copy()\n",
    "    for i in range(num_results):\n",
    "        current_prediction=predicted_results[i,:]\n",
    "        distances_to_vectors=[np.linalg.norm(current_prediction-my_vector) for my_vector in my_walsh_vectors]\n",
    "        minimum_dist_value=np.min(distances_to_vectors)\n",
    "        minimum_dist_ind=np.where(distances_to_vectors==minimum_dist_value)\n",
    "        my_index=minimum_dist_ind[0]\n",
    "        my_index=int(my_index[0])\n",
    "        new_prediction=my_walsh_vectors[my_index]\n",
    "        new_results[i,:]=new_prediction\n",
    "    return new_results\n",
    "\n",
    "def create_score(x,y):\n",
    "    #Score is based on accuracy. \n",
    "    #Accuracy= true predictions/all predictions\n",
    "    successful_pred=0\n",
    "    for i in range(x.shape[0]):\n",
    "        if (x[i,:]==y[i,:]).all():\n",
    "            successful_pred+=1\n",
    "    score=successful_pred/x.shape[0]\n",
    "    return score        \n",
    "#create Walsh\n",
    "def create_walsh(size_of_walsh):\n",
    "    # this fumction gives you a matrix whose rows are the vectors that can be used to represent classes. \n",
    "    #walsh vector representations allow classes to share some features. It is our best bet against class incremental learning. \n",
    "    #because previous tasks dynamically create features that can be exploited by future tasks. \n",
    "    size_of_walsh=int(math.log(size_of_walsh,2)+1)#2^n=size->>> n=log(size,2)\n",
    "    j=1\n",
    "    W_old=1\n",
    "    for i in range(size_of_walsh):\n",
    "        j_new=2**i\n",
    "        W_new=np.zeros([j_new,j_new], dtype=np.float32)\n",
    "        W_new[:j,:j]=W_old\n",
    "        W_new[j:,:j]=W_old\n",
    "        W_new[:j,j:]=W_old\n",
    "        W_new[j:,j:]=abs(1-W_old)\n",
    "        W_old=W_new\n",
    "        j=j_new\n",
    "    return W_new\n",
    "\n",
    "def calculate_bce_dist(y_train_i_predicted_mean, walsh_vec):\n",
    "    #we used Binary cross entropy in the model. So we will have to use it for class assignments as well.\n",
    "    #For the sake of consistency\n",
    "    epsilon = 1e-15  # Small value to prevent division by zero\n",
    "    loss_calc = - (y_train_i_predicted_mean * np.log(custom_sigmoid(walsh_vec + epsilon)) + (1 - y_train_i_predicted_mean) * np.log(1 - custom_sigmoid(walsh_vec + epsilon)))\n",
    "    caclulated_dist = np.mean(loss_calc)\n",
    "    return caclulated_dist\n",
    "\n",
    "def find_optimal_negotiation_plasticity_rate(neg):\n",
    "    \"\"\"\n",
    "    Lets derive the formula below:\n",
    "    We will consecutively allocate model's capacity to the tasks. But we have to make sure that each task uses the same amount.\n",
    "    Lets give an example\n",
    "    negotiation_rate=n=0.8\n",
    "    model's capacity that is allocated to the first task is 20 percent. \n",
    "    if we don't change negotiation rate,\n",
    "    model's capacity that is allocated to the second task will also be 20 percent.\n",
    "    But model's capacity that is allocated to the first task will decrease and become 16 percent. \n",
    "    Because 20 percent of 20 percent is equal to 4 percent. \n",
    "    so, we want to update the negotiation rate in such a way that ensures equal capacity allocation for each task. \n",
    "    \n",
    "    (1-n)*m=1-m\n",
    "    2*m=1-m*n\n",
    "    m(2+n)=1\n",
    "    m=1/(2+n)\n",
    "    the variable m denotes the new negotiation rate.\n",
    "    but we are trying to find a new variable which will be multiplied with n yielding to the new negotiation rate m. \n",
    "    new variable= m/n\n",
    "    \n",
    "    new variable=1/(2*n+n*n)\n",
    "    \n",
    "    new variable=1/(2*n+n^2)\n",
    "    \n",
    "    it can be seen that the formula below is exactly the same.\n",
    "    you should look at the line where this function is called. The explanation will make more sense. \n",
    "    \"\"\"\n",
    "    return 1/(2*neg-neg**2)\n",
    "\n",
    "def determine_walsh_vectors_as_output_labels(domain_model,x_train, y_train, available_representations):\n",
    "    \"\"\"\n",
    "    Expalanion:\n",
    "    In class incremental learning;\n",
    "    when we come across to a new class, we assign a class representative vector to that class. \n",
    "    We must select a class representative vector among the vectors that are not used. \n",
    "    For this purpose, we define a list called 'available_representations'.\n",
    "    We make sure that we don't use any of the vectors that are previously used for representing another class. \n",
    "    \n",
    "    In order to better exploit the model we first calculate output representations of all samples. \n",
    "    We then calculate the distances of all outputs to each available representation.\n",
    "    We then take the mean of the distances, so that we can compare all the mean values and \n",
    "    decide which representation is the closest, therefore the best, candidate for representing the new class. \n",
    "    we select the closest vector as a class representative update its index 'available_representations' to FALSE\n",
    "    by updating that index, we make sure that the selected index won't be used as the class representative for another class. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    y_train_matrix= np.full((x_train.shape[0],size_of_walsh),walsh_vector_list[0,:])\n",
    "    for i in np.unique(y_train):\n",
    "        x_train_i = x_train[y_train == i][:]        \n",
    "        # Compute the model prediction for this class and take the mean\n",
    "        y_train_i_predicted_mean = domain_model.predict(x_train_i).mean(axis=0)\n",
    "        # Initialize the closest vector to be None\n",
    "        closest_vector = None\n",
    "        closest_vector_index = None\n",
    "        min_distance = np.inf\n",
    "\n",
    "        # For each walsh vector, find the closest available walsh vector\n",
    "        for j, walsh_vec in enumerate(walsh_vector_list):\n",
    "            #if the best representation is not available try the next best until you hit an available representation. \n",
    "            if available_representations[j]:\n",
    "                \n",
    "                #dist = np.linalg.norm(y_train_i_predicted_mean - walsh_vec)\n",
    "                dist = calculate_bce_dist(y_train_i_predicted_mean, walsh_vec)\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    closest_vector = walsh_vec\n",
    "                    closest_vector_index = j\n",
    "\n",
    "        # If no vector is found, then all vectors have been assigned\n",
    "        if closest_vector is None:\n",
    "            raise Exception('All vectors have been assigned, we cannot assign new one for class ', i)\n",
    "\n",
    "        # Mark the vector as assigned\n",
    "        available_representations[closest_vector_index] = False\n",
    "\n",
    "        # Update the train matrix\n",
    "        y_train_matrix[y_train == i,:]= closest_vector\n",
    "    return y_train_matrix, available_representations\n",
    "\n",
    "\n",
    "average_of_all_seeds_list=[]\n",
    "initial_negotiation_rate_list=[]\n",
    "#for initial_negotiation_rate in range(1,99):\n",
    "initial_negotiation_rate=93\n",
    "#Experiments....\n",
    "acc_lists=[]# average accuracies belonging to each seed are stored in here.\n",
    "negotiation_rate=initial_negotiation_rate*0.01 #initial negotiation rate.\n",
    "initial_negotiation_rate_list.append(negotiation_rate)\n",
    "for my_seed in range(5):\n",
    "    #we do the experiments with different seed values. Just to make sure the results are not accidental. \n",
    "    # we can also use this setup to make sure the variance is not high.\n",
    "    #which means that the accuracy should not change very much by changing the seed. \n",
    "    os.environ['PYTHONHASHSEED']=str(1)\n",
    "    tf.random.set_seed(my_seed)\n",
    "    np.random.seed(my_seed)\n",
    "    random.seed(my_seed)\n",
    "    #settings\n",
    "    sigmoid_softener=True\n",
    "    sigmoid_softening_coeff=5# linearization coefficient of the sigmoid function. \n",
    "    negotiation_rate=initial_negotiation_rate*0.01 #initial negotiation rate.\n",
    "    batch_size = 128\n",
    "    size_of_walsh=128#size of each walsh vector. \n",
    "    epochs = 10#number of epochs\n",
    "    num_classes=100#CIFAR100\n",
    "\n",
    "    #Divnet related functions\n",
    "    x_train_class = []\n",
    "    x_test_class = []\n",
    "    y_test_class = []\n",
    "    y_train_class = []\n",
    "    scores_list=[]\n",
    "    selected_vectors=[]\n",
    "    x_train_classes=[]\n",
    "    y_train_classes=[]\n",
    "    x_test_classes=[]\n",
    "    y_test_classes=[]\n",
    "\n",
    "    y_train_map_list=[]\n",
    "\n",
    "    #num_pairs=10\n",
    "    # Create 10 class pairs for CIFAR-100\n",
    "    #class_pairs = [tuple(range(i,i+int(num_classes/num_pairs))) for i in range(0, num_classes, int(num_classes/num_pairs))]\n",
    "    #class_pairs = [[i for i in range(start, start + 10)] for start in range(0, 100, 10)]\n",
    "    class_pairs = [(i for i in range(start, start + 20)) for start in range(0, 100, 20)]\n",
    "    \"\"\"\n",
    "    class_pairs= [(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19),\n",
    "                  (20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39),\n",
    "                  (40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59),\n",
    "                  (60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79),\n",
    "                  (80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99)]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CIFAR-100 dataset\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "    y_train, y_test = y_train.flatten(), y_test.flatten()\n",
    "\n",
    "    # Normalize the data\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "    possible_assignments=[]\n",
    "    # Walsh vector encoding\n",
    "    walsh_vector_list=create_walsh(size_of_walsh)#the matrix the rows of which will be selected as vector representations. \n",
    "\n",
    "    print(\"walsh_vector_list.shape\",walsh_vector_list.shape)\n",
    "    available_representations=[True] * size_of_walsh\n",
    "\n",
    "\n",
    "    # Main loop\n",
    "    model = create_model()\n",
    "    x_test_pairs=[]\n",
    "    y_test_pairs=[]\n",
    "    for i, class_pair in enumerate(class_pairs):\n",
    "        #print(f\"Training on class pair {class_pair}\")\n",
    "        for my_class in class_pair:\n",
    "            #print(\"My class: \",my_class)\n",
    "            x_train_my_class=x_train[y_train==my_class,:,:]\n",
    "            y_train_my_class=y_train[y_train==my_class]\n",
    "            x_test_my_class=x_test[y_test==my_class,:,:]\n",
    "            y_test_my_class=y_test[y_test==my_class]\n",
    "            # Assign each label in y_train and y_test to its corresponding 1xsize_of_walsh vector in the matrix\n",
    "\n",
    "            y_train_my_class_mapped, available_representations = determine_walsh_vectors_as_output_labels(model,x_train_my_class,y_train_my_class, available_representations)\n",
    "            y_test_my_class_mapped = np.full((x_test_my_class.shape[0],size_of_walsh),y_train_my_class_mapped[0,:])\n",
    "            possible_assignments.append(np.unique(y_train_my_class_mapped,axis=0))\n",
    "            x_train_classes.append(x_train_my_class)\n",
    "            y_train_classes.append(y_train_my_class_mapped)\n",
    "            x_test_classes.append(x_test_my_class)\n",
    "            y_test_classes.append(y_test_my_class_mapped)  \n",
    "\n",
    "        x_train_pair=np.concatenate((x_train_classes[-1],x_train_classes[-2]),axis=0)\n",
    "        y_train_pair=np.concatenate((y_train_classes[-1],y_train_classes[-2]),axis=0)\n",
    "        x_test_pair=np.concatenate((x_test_classes[-1],x_test_classes[-2]),axis=0)\n",
    "        y_test_pair=np.concatenate((y_test_classes[-1],y_test_classes[-2]),axis=0)\n",
    "        y_pred = model.predict(x_train_pair)\n",
    "        # Negotiate with the past experiences in order to not forget previous tasks. \n",
    "        #y_train_pair=y_train_pair*(1-negotiation_rate)+y_pred*negotiation_rate\n",
    "        #optimal plasticity for the allocation of equal amount of model's capacity for all the tasks. \n",
    "        negotiation_plasticity_rate=find_optimal_negotiation_plasticity_rate(negotiation_rate)\n",
    "        negotiation_rate=negotiation_rate*negotiation_plasticity_rate\n",
    "        #print(\"negotiation_rate:\",negotiation_rate)\n",
    "        model.fit(x_train_pair, y_train_pair, epochs=epochs, verbose=0,shuffle=True)\n",
    "        x_test_pairs.append(x_test_pair)\n",
    "        y_test_pairs.append(y_test_pair)\n",
    "        acc_list=[]\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        #EVALUATE\n",
    "        for iii, x_test_pair in enumerate(x_test_pairs):\n",
    "            y_test_pair_classes=y_test_pairs[iii]\n",
    "            y_pred = model.predict(x_test_pair)\n",
    "            y_pred_classes = assign_to_new_classes(y_pred,possible_assignments)\n",
    "            accuracy = create_score(y_pred_classes, y_test_pair_classes)\n",
    "            #print(f\"Accuracy for class pair {iii}: {accuracy:.4f}\")\n",
    "            #print(\"\\n\")\n",
    "            acc_list.append(accuracy)\n",
    "    average_acc=sum(acc_list)/len(acc_list)\n",
    "    print(\"average_acc of all classes for the current seed\",average_acc)\n",
    "    acc_lists.append(average_acc)\n",
    "\n",
    "average_of_all_seeds=sum(acc_lists)/len(acc_lists)\n",
    "print(\"average_of_all_seeds:\",average_of_all_seeds)\n",
    "print(\"individiaul_averages:\",acc_lists)\n",
    "\n",
    "\n",
    "average_of_all_seeds=sum(acc_lists)/len(acc_lists)\n",
    "average_of_all_seeds_list.append(average_of_all_seeds)\n",
    "print(\"average_of_all_seeds:\",average_of_all_seeds)\n",
    "print(\"individiaul_averages:\",acc_lists)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff1e89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(len(average_of_all_seeds_list))\\nprint(len(initial_negotiation_rate_list))\\n\\nimport matplotlib.pyplot as plt\\n\\n# Sample data\\n#average_of_all_seeds_list = [1, 2, 3, 4, 5]\\n\\n# Creating x-axis values (assuming the list is to be plotted against indices)\\nx_values = list(range(len(average_of_all_seeds_list)))\\n# Set the figure size\\nplt.figure(figsize=(10, 10))\\n\\n# Plotting the list\\nplt.plot(initial_negotiation_rate_list, average_of_all_seeds_list, marker='o', linestyle='-')\\n\\n#plt.plot( average_of_all_seeds_list)\\n# Adding title and labels\\nplt.title('Average of All Seeds')\\nplt.xlabel('Initial Negotiation Rate')\\nplt.ylabel('Average of 5 seeds that include the average accuracy of 5 tasks')\\nplt.legend()\\n# Display the plot\\nplt.show()\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(len(average_of_all_seeds_list))\n",
    "print(len(initial_negotiation_rate_list))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "#average_of_all_seeds_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Creating x-axis values (assuming the list is to be plotted against indices)\n",
    "x_values = list(range(len(average_of_all_seeds_list)))\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Plotting the list\n",
    "plt.plot(initial_negotiation_rate_list, average_of_all_seeds_list, marker='o', linestyle='-')\n",
    "\n",
    "#plt.plot( average_of_all_seeds_list)\n",
    "# Adding title and labels\n",
    "plt.title('Average of All Seeds')\n",
    "plt.xlabel('Initial Negotiation Rate')\n",
    "plt.ylabel('Average of 5 seeds that include the average accuracy of 5 tasks')\n",
    "plt.legend()\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "648cbcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_notebook()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25388991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bdd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f25b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
